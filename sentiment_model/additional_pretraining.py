# -*- coding: utf-8 -*-
"""AdditionalPretraining_KcBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16fgazxhbQIFG8SzfXNw9uIG9XJd24ANR
"""

#prepare data with create_pretraining_data.py

#run run_pretraining.py setting the init_checkpoint to the downloaded model. setting lr lower

!pip install sentencepiece
!git clone https://github.com/google-research/bert

# !pip uninstall tensorflow
# !pip install tensorflow==1.12.0

import os
import sys
import json
import nltk
import random
import logging
import tensorflow as tf
import sentencepiece as spm

from glob import glob
from google.colab import auth, drive
from tensorflow.keras.utils import Progbar

sys.path.append("bert")

from bert import modeling


# auth.authenticate_user()
  
# configure logging
log = logging.getLogger('tensorflow')
log.setLevel(logging.INFO)

# create formatter and add it to the handlers
formatter = logging.Formatter('%(asctime)s :  %(message)s')
sh = logging.StreamHandler()
sh.setLevel(logging.INFO)
sh.setFormatter(formatter)
log.handlers = [sh]

# if 'COLAB_TPU_ADDR' in os.environ:
#   log.info("Using TPU runtime")
#   USE_TPU = True
#   TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']

#   with tf.Session(TPU_ADDRESS) as session:
#     log.info('TPU address is ' + TPU_ADDRESS)
#     # Upload credentials to TPU.
#     with open('/content/adc.json', 'r') as f:
#       auth_info = json.load(f)
#     tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
    
# else:
#   log.warning('Not connected to TPU runtime')
#   USE_TPU = False

# Load the TensorBoard notebook extension
!pip install tensorboardX
from tensorboardX import SummaryWriter
writer = SummaryWriter()

# 구글 드라이브에서 데이터 로드
drive.mount('/content/drive', force_remount=True)

# PRC_DATA_FPATH = "/content/drive/My\ Drive/Colab\ Notebooks/상담문장.txt"
# !mkdir ./shards
# !split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_
# !ls ./shards/

# MAX_SEQ_LENGTH = 128 #@param {type:"integer"}
# MASKED_LM_PROB = 0.15 #@param
# MAX_PREDICTIONS = 20 #@param {type:"integer"}
# DO_LOWER_CASE = False #@param {type:"boolean"}
# PROCESSES = 2 #@param {type:"integer"}
# PRETRAINING_DIR = "pretraining_data" #@param {type:"string"}

# VOC_FNAME = "vocab.txt"
# XARGS_CMD = ("ls ./shards/ | "
#              "xargs -n 1 -P {} -I{} "
#              "python3 bert/create_pretraining_data.py "
#              "--input_file=./shards/{} "
#              "--output_file={}/{}.tfrecord "
#              "--vocab_file={} "
#              "--do_lower_case={} "
#              "--max_predictions_per_seq={} "
#              "--max_seq_length={} "
#              "--masked_lm_prob={} "
#              "--random_seed=34 "
#              "--dupe_factor=5")

# XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', 
#                              VOC_FNAME, DO_LOWER_CASE, 
#                              MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)

# import tensorflow as tf
# tf.gfile.MkDir(PRETRAINING_DIR)
# !$XARGS_CMD

# BUCKET_NAME = "bert_model_te" #@param {type:"string"}
# MODEL_DIR = "bert_model" #@param {type:"string"}
# tf.gfile.MkDir(MODEL_DIR)

# if not BUCKET_NAME:
#   log.warning("WARNING: BUCKET_NAME is not set. "
#               "You will not be able to train the model.")

# bert_base_config = {
#     "type_vocab_size": 2,
#     "initializer_range": 0.02,
#     "max_position_embeddings": 300,
#     "vocab_size": 30000,
#     "hidden_size": 1024,
#     "hidden_dropout_prob": 0.1,
#     "model_type": "bert",
#     "directionality": "bidi",
#     "pooler_num_attention_heads": 12,
#     "pooler_fc_size": 768,
#     "pad_token_id": 0,
#     "pooler_type": "first_token_transform",
#     "layer_norm_eps": 1e-12,
#     "hidden_act": "gelu",
#     "num_hidden_layers": 24,
#     "pooler_num_fc_layers": 3,
#     "num_attention_heads": 16,
#     "pooler_size_per_head": 128,
#     "attention_probs_dropout_prob": 0.1,
#     "intermediate_size": 4096,
#     "architectures": [
#         "BertForMaskedLM"
#     ]
# }

# with open("{}/bert_config.json".format(MODEL_DIR), "w") as fo:
#   json.dump(bert_base_config, fo, indent=2)
  
# # with open("{}/{}".format(MODEL_DIR, VOC_FNAME), "w") as fo:
# #   for token in bert_vocab:
# #     fo.write(token+"\n")
# if BUCKET_NAME:
#   !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME

!mkdir CounselKcBERT
!pip install git+https://github.com/huggingface/transformers
from transformers import AutoTokenizer, AutoModelWithLMHead

tokenizer = tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-base")
model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-base")

!nvidia-smi
import torch
torch.cuda.is_available()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from transformers import LineByLineTextDataset
# 
# dataset = LineByLineTextDataset(
#     tokenizer=tokenizer,
#     file_path= "/content/drive/My Drive/Colab Notebooks/상담문장_최종.txt",
#     block_size=128,
# )

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./CounselKcBERT",
    overwrite_output_dir=True,
    num_train_epochs=4,
    per_gpu_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate = 2e-5
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
    prediction_loss_only=True,
    tb_writer = writer
)

model.num_parameters()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trainer.train()

# Commented out IPython magic to ensure Python compatibility.
import datetime
from tensorflow import summary
# %load_ext tensorboard
writer.flush()
writer.close()
import os
logs_base_dir = "runs"
os.makedirs(logs_base_dir, exist_ok=True)
# %tensorboard --logdir {logs_base_dir}

trainer.save_model( "/content/drive/My Drive/CounselKcBERT2" )

# !gcloud config set project trueeducation-stt
# !gsutil cp -r gs://bert_model_te ./

import os
import tensorflow as tf

BUCKET_NAME = "bert_model_te" #@param {type:"string"}
MODEL_DIR = "bert_model" #@param {type:"string"}
PRETRAINING_DIR = "pretraining_data" #@param {type:"string"}
VOC_FNAME = "vocab.txt" #@param {type:"string"}

# Input data pipeline config
TRAIN_BATCH_SIZE = 128 #@param {type:"integer"}
MAX_PREDICTIONS = 20 #@param {type:"integer"}
MAX_SEQ_LENGTH = 128 #@param {type:"integer"}
MASKED_LM_PROB = 0.15 #@param

# Training procedure config
EVAL_BATCH_SIZE = 64
LEARNING_RATE = 2e-5
TRAIN_STEPS = 1000000 #@param {type:"integer"}
SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:"integer"}
NUM_TPU_CORES = 8

if BUCKET_NAME:
  BUCKET_PATH = "gs://{}".format(BUCKET_NAME)
else:
  BUCKET_PATH = "."

BERT_GCS_DIR = "{}/{}".format(BUCKET_PATH, MODEL_DIR)
DATA_GCS_DIR = "{}/{}".format(BUCKET_PATH, PRETRAINING_DIR)

VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)
CONFIG_FILE = os.path.join(BERT_GCS_DIR, "bert_config.json")

INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)

bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)
input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))

log.info("Using checkpoint: {}".format(INIT_CHECKPOINT))
log.info("Using {} data shards".format(len(input_files)))

model_fn = model_fn_builder(
      bert_config=bert_config,
      init_checkpoint=INIT_CHECKPOINT,
      learning_rate=LEARNING_RATE,
      num_train_steps=TRAIN_STEPS,
      num_warmup_steps=10,
      use_tpu=USE_TPU,
      use_one_hot_embeddings=True)

tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)

run_config = tf.contrib.tpu.RunConfig(
    cluster=tpu_cluster_resolver,
    model_dir=BERT_GCS_DIR,
    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,
    tpu_config=tf.contrib.tpu.TPUConfig(
        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,
        num_shards=NUM_TPU_CORES,
        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))

estimator = tf.contrib.tpu.TPUEstimator(
    use_tpu=USE_TPU,
    model_fn=model_fn,
    config=run_config,
    train_batch_size=TRAIN_BATCH_SIZE,
    eval_batch_size=EVAL_BATCH_SIZE)
  
train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=MAX_SEQ_LENGTH,
        max_predictions_per_seq=MAX_PREDICTIONS,
        is_training=True)

!pip uninstall tensorflow
!pip install tensorflow

import tensorflow as tf
print(tf.__version__)



estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)

